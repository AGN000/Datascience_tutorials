{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1Ô∏è‚É£ Imports and Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "from transformer_blocks import Block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1\n",
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2Ô∏è‚É£ Create a Small Text Corpus (New Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love south indian food <END> we traveled by train to chennai <END> the dosa was crispy and tasty <END> tea tastes better in the evening <END> we visited the beach at sunrise <END> spicy food makes me happy <END> the journey was long but fun <END> coffee and snacks are perfect <END>\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"i love south indian food\",\n",
    "    \"we traveled by train to chennai\",\n",
    "    \"the dosa was crispy and tasty\",\n",
    "    \"tea tastes better in the evening\",\n",
    "    \"we visited the beach at sunrise\",\n",
    "    \"spicy food makes me happy\",\n",
    "    \"the journey was long but fun\",\n",
    "    \"coffee and snacks are perfect\",\n",
    "]\n",
    "\n",
    "# Add end-of-sentence token\n",
    "corpus = [s + \" <END>\" for s in corpus]\n",
    "\n",
    "text = \" \".join(corpus)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3Ô∏è‚É£ Vocabulary Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['coffee', 'in', 'chennai', 'makes', 'south', 'we', 'train', 'tasty', 'crispy', 'i', 'the', 'long', 'perfect', 'better', 'food', 'indian', 'beach', 'snacks', 'are', 'was', 'me', 'to', 'visited', 'traveled', '<END>', 'and', 'tastes', 'love', 'by', 'evening', 'tea', 'at', 'fun', 'but', 'journey', 'happy', 'spicy', 'sunrise', 'dosa']\n",
      "Vocab size: 39\n"
     ]
    }
   ],
   "source": [
    "words = list(set(text.split()))\n",
    "vocab_size = len(words)\n",
    "\n",
    "print(\"Vocabulary:\", words)\n",
    "print(\"Vocab size:\", vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2idx: {'coffee': 0, 'in': 1, 'chennai': 2, 'makes': 3, 'south': 4, 'we': 5, 'train': 6, 'tasty': 7, 'crispy': 8, 'i': 9, 'the': 10, 'long': 11, 'perfect': 12, 'better': 13, 'food': 14, 'indian': 15, 'beach': 16, 'snacks': 17, 'are': 18, 'was': 19, 'me': 20, 'to': 21, 'visited': 22, 'traveled': 23, '<END>': 24, 'and': 25, 'tastes': 26, 'love': 27, 'by': 28, 'evening': 29, 'tea': 30, 'at': 31, 'fun': 32, 'but': 33, 'journey': 34, 'happy': 35, 'spicy': 36, 'sunrise': 37, 'dosa': 38}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "\n",
    "print(\"word2idx:\", word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4Ô∏è‚É£ Encode Text as Token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data: tensor([ 9, 27,  4, 15, 14, 24,  5, 23, 28,  6, 21,  2, 24, 10, 38, 19,  8, 25,\n",
      "         7, 24, 30, 26, 13,  1, 10, 29, 24,  5, 22, 10, 16, 31, 37, 24, 36, 14,\n",
      "         3, 20, 35, 24, 10, 34, 19, 11, 33, 32, 24,  0, 25, 17, 18, 12, 24])\n",
      "Total tokens: 53\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor([word2idx[w] for w in text.split()], dtype=torch.long)\n",
    "\n",
    "print(\"Encoded data:\", data)\n",
    "print(\"Total tokens:\", len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5Ô∏è‚É£ Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 6\n",
    "embedding_dim = 32\n",
    "n_heads = 2\n",
    "n_layers = 2\n",
    "learning_rate = 1e-3\n",
    "epochs = 1500\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6Ô∏è‚É£ Mini-batch Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1:i + block_size + 1] for i in ix])\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7Ô∏è‚É£ TinyGPT Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyGPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_embedding = nn.Embedding(block_size, embedding_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block(embedding_dim, block_size, n_heads) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        self.ln_f = nn.LayerNorm(embedding_dim)\n",
    "        self.head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding(idx)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))\n",
    "\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            loss = F.cross_entropy(\n",
    "                logits.view(B * T, C),\n",
    "                targets.view(B * T)\n",
    "            )\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            next_idx = torch.multinomial(probs, 1)\n",
    "            idx = torch.cat((idx, next_idx), dim=1)\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8Ô∏è‚É£ Model Initialization and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 28007\n"
     ]
    }
   ],
   "source": [
    "model = TinyGPT()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(\"Total parameters:\", sum(p.numel() for p in model.parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9Ô∏è‚É£ Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss = 3.7808\n",
      "Step 300, Loss = 0.1283\n",
      "Step 600, Loss = 0.1144\n",
      "Step 900, Loss = 0.0849\n",
      "Step 1200, Loss = 0.1054\n"
     ]
    }
   ],
   "source": [
    "for step in range(epochs):\n",
    "    xb, yb = get_batch()\n",
    "    logits, loss = model(xb, yb)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 300 == 0:\n",
    "        print(f\"Step {step}, Loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## üîü Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "\n",
      "i love south indian food <END> we traveled by train to chennai <END> the dosa was crispy and tasty <END> tea tastes better in the evening <END> we visited the beach\n"
     ]
    }
   ],
   "source": [
    "start_word = \"i\"\n",
    "context = torch.tensor([[word2idx[start_word]]], dtype=torch.long)\n",
    "\n",
    "generated = model.generate(context, max_new_tokens=30)\n",
    "\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(\" \".join(idx2word[int(i)] for i in generated[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
