{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9016541",
   "metadata": {},
   "source": [
    "# Mini Tutorial: Tokenization in NLP\n",
    "This notebook demonstrates different tokenization strategies with examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1b677d",
   "metadata": {},
   "source": [
    "## Example sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d549041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"Learning turbulence models is hard!\"\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832b923",
   "metadata": {},
   "source": [
    "## 1. Word-level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b3d14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def word_tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "\n",
    "print(word_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1eaac2",
   "metadata": {},
   "source": [
    "## 2. Character-level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5be129",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def char_tokenize(text):\n",
    "    return list(text)\n",
    "\n",
    "print(char_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af54ec",
   "metadata": {},
   "source": [
    "## 3. Toy BPE-style Tokenization (simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c6e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bpe_tokenize(text, vocab):\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(text):\n",
    "        for j in range(len(text), i, -1):\n",
    "            sub = text[i:j]\n",
    "            if sub in vocab:\n",
    "                tokens.append(sub)\n",
    "                i = j\n",
    "                break\n",
    "        else:\n",
    "            tokens.append(text[i])\n",
    "            i += 1\n",
    "    return tokens\n",
    "\n",
    "vocab = {\"Learning\", \" turbulence\", \" models\", \" is\", \" hard\", \"!\"}\n",
    "print(bpe_tokenize(text, vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460d4848",
   "metadata": {},
   "source": [
    "## 4. Byte-level Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def byte_tokenize(text):\n",
    "    return list(text.encode(\"utf-8\"))\n",
    "\n",
    "print(byte_tokenize(\"Hi!\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4f7a2e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Word-level: simple but OOV-prone\n",
    "- Character-level: robust but long sequences\n",
    "- BPE: efficient subwords (GPT-2/3)\n",
    "- SentencePiece: multilingual (Gemini, LLaMA)\n",
    "- Byte-level: most robust (GPT-4, Grok)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
